# -*- coding: utf-8 -*-
"""Depression_detection latest.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/112FlcVlNmSeg5vXQOlr3Id2LY0_gmIz8
"""

import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import re

from google.colab import drive
drive.mount('/content/drive')

df1 = pd.read_excel('/content/drive/My Drive/finalpreprocessed.xlsx',header=0,index_col=None,
keep_default_na=True)
df1.head()
len(df1)

df1.head()

df=df1.dropna()

df.replace(to_replace='Non-Offensive', value='Non-offensive', inplace=True)

df['Label'].unique()

df.columns

df['Label'] = df.Label.replace(to_replace=['Non-offensive', 'Offensive'], value=[0, 1])
# 0 - Not offensive
# 1 - Offensive

df.head()

df.Text[0]

def deEmojify(text):
    regrex_pattern = re.compile(pattern = "["
        u"\U0001F600-\U0001F64F"  # emoticons
        u"\U0001F300-\U0001F5FF"  # symbols & pictographs
        u"\U0001F680-\U0001F6FF"  # transport & map symbols
        u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
                           "]+", flags = re.UNICODE)
    return regrex_pattern.sub(r'',text)

import requests

link = "https://raw.githubusercontent.com/AshokR/TamilNLP/master/tamilnlp/Resources/TamilStopWords.txt"
req = requests.get(link)
stopwords = req.text

def clean_text(text):

  corpus = ''.join(str(j) for j in text)
  corpus = corpus.replace('.','')
  corpus = corpus.replace('?','')
  corpus = corpus.replace(',','')
  corpus = corpus.replace('!','')
  corpus = corpus.replace('/','')
  corpus = corpus.replace('@','')
  corpus = corpus.replace('\\','')
  corpus = corpus.replace('(','')
  corpus = corpus.replace(')','')
  corpus = corpus.replace('-','')
  corpus = corpus.replace('_','')
  corpus = corpus.replace('@','')
  corpus = corpus.replace('#','')
  corpus = corpus.replace('$','')
  corpus = corpus.replace(';','')
  corpus = corpus.replace(':','')
  corpus = corpus.replace('\'','')
  corpus = corpus.replace('\"','')
  corpus = corpus.replace('*','')
  corpus = ''.join(c for c in corpus if not c.isdigit())
  corpus = deEmojify(corpus)
  corpus = corpus.split()
  corpus = [word for word in corpus if word not in str(stopwords.split()) and not word.isalpha()]
  corpus = ' '.join(corpus)
  return corpus

for row in df:
  df['Text'] = df['Text'].astype(str)
  df['Text'] = df['Text'].apply(clean_text)

df.head()

df.shape

df_ngram.to_csv('/content/drive/My Drive/Final bigrams.csv')

import re
from collections import Counter
from nltk.util import ngrams
text_file = open('/content/drive/My Drive/finaldatasetlatest.txt', 'r')
text = text_file.read()
n_gram = 2
Counter(ngrams(text.split(), n_gram))

import nltk
from nltk import FreqDist
from nltk.util import ngrams
f = open('/content/drive/My Drive/finaldatasetlatest.txt')
raw=f.read()
d = dict()
tokens = nltk.word_tokenize(raw)

#Create your bigrams
bgs = nltk.bigrams(tokens)

#compute frequency distribution for all the bigrams in the text
fdist = nltk.FreqDist(bgs)
for k,v in fdist.items():
    word_count.write(str((k,v)))
    word_count.write("\n")
  #word_count.write(":")
  #word_count.write(str(v))
  #word_count.write("\n")
  #print (k,v)


file1.close()

nltk.download('punkt')

dtm.shape

word_count= open('/content/drive/My Drive/bigram_countfinal.txt','w')

vect.get_feature_names()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(dtm, df['Label'], test_size=.25)

X_train.shape, y_train.shape, X_test.shape, y_test.shape

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(X_train.toarray(), y_train)
nb_pred_train = gnb.predict(X_train.toarray())
nb_pred_test = gnb.predict(X_test.toarray())

nb_pred_test

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test,nb_pred_test)

from sklearn.metrics import accuracy_score
accuracy_score(y_test,nb_pred_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,nb_pred_test))

from sklearn.metrics import f1_score
f1_score(y_test,nb_pred_test,average='weighted')

test_df = pd.read_excel('/content/drive/My Drive/finalpreprocessed test.xlsx',header=0,index_col=None,
keep_default_na=True)
test_df.head()

test_df.shape

test_df.columns

test_df.head()

for row in test_df:
  test_df['Text'] = test_df['Text'].astype(str)
  test_df['Text']= test_df['Text'].apply(clean_text)

test_df.rename(columns={'id\ttext': 'text'},inplace=True)

test_vect = TfidfVectorizer(ngram_range=(1,1), max_features=192)
test_dtm = test_vect.fit_transform(test_df.Text)

test_dtm.shape,dtm.shape

pred_test = gnb.predict(test_dtm.toarray())

test_df['Prediction'] = pred_test

test_df.head()

test_df.to_csv('/content/drive/My Drive/Final_test-Results.csv')

df_final = pd.read_excel('/content/drive/My Drive/Copy of finalpreprocessed test with label.xlsx',header=0,index_col=None,
keep_default_na=True)
df_final.head()

df_final['Label'] = df_final.Label.replace(to_replace=['Non-offensive', 'Offensive'], value=[0, 1])

y_test1=df_final['Label']

from sklearn.metrics import confusion_matrix
confusion_matrix(y_test1,pred_test)